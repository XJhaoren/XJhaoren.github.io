<!DOCTYPE HTML>
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta name="author" content="Jian Xu">
  <meta name="description" content="Jian Xu's Homepage">
  <meta name="keywords" content="Jian Xu,徐健,homepage,主页,PhD,Large Multimodal Models, AI4Science, Pose Estimation, Image Retrieval">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Jian Xu (徐健)'s Homepage</title>
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:70%;vertical-align:middle">
              <p style="text-align:center">
                <name>Jian Xu (徐健)</name>
              </p>
              <p style="text-align:center">
                <span>&#9993;</span> jian.xu@ia.ac.cn &nbsp;
              </p>
              <p style="text-align:left">
                <a href="https://scholar.google.com.hk/citations?user=hqyYoO0AAAAJ&hl=zh-CN"><img src="asserts/google.png" height="40px"></a> &nbsp;&nbsp;
                <a href="https://github.com/XJhaoren"><img src="asserts/github.png" height="40px"></a> &nbsp;&nbsp;
                <!-- <a href="asserts/CV.pdf"><img src="asserts/resume.png" height="40px"></a> &nbsp; -->
              </p>
              <p>I am currently an Associate Professor at Institute of Automation Chinese Academy of Sciences (<a href="http://www.ia.cas.cn/">CASIA</a>) in <a href="https://nlpr.ia.ac.cn/pal/index.html">PAL group</a>.
              </p>
              <p>Before joining CASIA, I have 3 years of experience in AI corporations HUAWEI and XREAL.
              </p>
              <p>
                I obtained my Ph.D. in Pattern Recognition and Intelligent Systems from Institute of Automation, Chinese Academy of Science in 2020. 
                Previously I received my B.S. in Control Science and Engineering from Shandong University in 2015.
              </p>
              <p>
                <!-- I have been focusing on object detection/tracking since I started my PhD in 2015.  -->
                My research interests lie in Large Multimodal Models, AI4Science, Pose Estimation and Image Retrieval.
                </p>
              <p>
                <!-- <strong>
                I'm seeking research interns on 3D human representation. Feel free to send me an email if you are interested.
                </strong> -->
              </p>
		    
            </td>
            <td style="padding:0% 0% 0% 0%;text-align:center;width:30%;max-width:30%";vertical-align:middle>
              <a href="asserts/jpg"><img style="width:80%;max-width:80%" alt="profile photo" src="asserts/protrait.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:0% 0% 0% 0%;text-align:center;width:30%;max-width:30%">
            <a href="asserts/logo.png"><img style="width:80%;max-width:80%" alt="profile photo" src="asserts/logo.png" class="hoverZoomLink"></a>
          </td>
        </tr>
        </tbody></table>
        
        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <strong>2023/08/25</strong> &nbsp Had one papers accepted by IEEE Transactions on Image Processing about visual restoration.
              </p><p>
              <p>
                <strong>2023/07/14</strong> &nbsp Had two papers accepted by ICCV 2023 about 3D-aware GAN and face swap.
              </p><p>
              <p>
                <strong>2023/02/28</strong> &nbsp Had one paper accepted by CVPR 2023 about neural hand rendering.
              </p><p>
              <p>
                <strong>2022/07/09</strong> &nbsp Had one paper accepted by ECCV 2022 about open-world object detection.
              </p><p>
              <p>
                <strong>2022/06/30</strong> &nbsp Had one paper accepted by ACM MM 2022 about hand mesh reconstruction.
              </p><p>
                <strong>2022/04/02</strong> &nbsp Our <a href="https://www.amazon.com/Visual-Perception-Control-Underwater-Robots-dp-0367695782/dp/0367695782/ref=mt_other?_encoding=UTF8&me=&qid=">book</a> was <strong>Highly Recommended</strong> by <a href="https://www.choice360.org/products/choice-reviews/">CHOICE Reviews</a>.              
              </p><p>
                <strong>2022/03/02</strong> &nbsp Had one paper accepted by CVPR 2022 about monocular hand reconstruction.              
              </p><p>
                <strong>2022/02/25</strong> &nbsp Had one paper accepted by IEEE Transactions on Cybernetics about few-shot object detection.              
              </p><p>
                <strong>2021/09/01</strong> &nbsp  <a href="https://github.com/SeanChenxy/Hand3DResearch">Hand3DResearch</a> was released to track recent works in 3D hand tasks.              
              </p><p>
                <strong>2021/03/01</strong> &nbsp Had one paper accepted by CVPR 2021 about monocular hand reconstruction.              
              </p>
            </td>
          </tr>
        </tbody></table> -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Publications</heading>
<!--             <p>
              (<a href="publication.html">here</a> for full publications)
            </p> -->
          </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


	<tr></tr>
	  <td style="padding:20px;width:25%;vertical-align:middle">
		<div class="one" >
		  <img src='asserts/PDE-Agent.png' style="width:100%;max-width:100%; position: absolute;top: +20%">
		</div>
	  </td>
	  <td style="padding:20px;width:75%;vertical-align:middle">
		  <papertitle>[18] PDE-Agent: A toolchain-augmented multi-agent framework for PDE solving</papertitle>
		  <br>
		  <i class="fa fa-envelope"></i>
		  Jianming Liu, Ren Zhu,  <strong>Jian Xu</strong>, Kun Ding, Xu-Yao Zhang, Gaofeng Meng, Cheng-Lin Liu
		  <br>
		  arXiv 2025
		  <br>
		  <a href="https://arxiv.org/abs/2512.16214">[PDF]</a>
		  <a href="https://github.com/ScienceOne-AI/PDE-Agent">[Project]</a>
		  <br>
	  </td>
	</tr>	
			
			
	<tr></tr>
	  <td style="padding:20px;width:25%;vertical-align:middle">
		<div class="one" >
		  <img src='asserts/MDSTNet.png' style="width:100%;max-width:100%; position: absolute;top: +20%">
		</div>
	  </td>
	  <td style="padding:20px;width:75%;vertical-align:middle">
		  <papertitle>[17] Air Quality Prediction with A Meteorology-Guided Modality-Decoupled Spatio-Temporal Network</papertitle>
		  <br>
		  <i class="fa fa-envelope"></i>
		  Hang Yin, Yan-Ming Zhang, <strong>Jian Xu</strong>, Jian-Long Chang, Yin Li, Cheng-Lin Liu
		  <br>
		  IEEE Transactions on Artificial Intelligence, 2025
		  <br>
		  <a href="https://github.com/dylan-yin/MDSTNet">[Project]</a>
		  <br>
	  </td>
	</tr>	

	<tr></tr>
	  <td style="padding:20px;width:25%;vertical-align:middle">
		<div class="one" >
		  <img src='asserts/Scientific Agents.png' style="width:100%;max-width:100%; position: absolute;top: +5%">
		</div>
	  </td>
	  <td style="padding:20px;width:75%;vertical-align:middle">
		  <papertitle>[16] The Hitchhiker’s Guide to Autonomous Research: A Survey of Scientific Agents</papertitle>
		  <br>
		  <i class="fa fa-envelope"></i>
		  Xinming Wang, <strong>Jian Xu</strong>, Aslan H. Feng, Yi Chen, Haiyang Guo, Fei Zhu, Yuanqi Shao, Minsi Ren, Hongzhu Yi, Sheng Lian, Hongming Yang, Tailin Wu, Han Hu, Shiming Xiang, Xu-Yao Zhang, Cheng-Lin Liu
		  <br>
		  <a href="https://doi.org/10.36227/techrxiv.175459840.02185500/v1">[PDF]</a>
		  <a href="https://github.com/gudehhh666/Awesome_Scientific_Agent.git">[Project]</a>
		  <br>
	  </td>
	</tr>	

	<tr></tr>
	  <td style="padding:20px;width:25%;vertical-align:middle">
		<div class="one" >
		  <img src='asserts/poem_pami2025.png' style="width:100%;max-width:100%; position: absolute;top: +30%">
		</div>
	  </td>
	  <td style="padding:20px;width:75%;vertical-align:middle">
		  <papertitle>[15] Multi-view Hand Reconstruction with a Point-Embedded Transformer</papertitle>
		  <br>
		  <i class="fa fa-envelope"></i>
		  Lixin Yang, Licheng Zhong, Pengxiang Zhu, Xinyu Zhan, Junxiao Kong, <strong>Jian Xu</strong>, Cewu Lu
		  <br>
		  IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025
		  <br>
	  </td>
	</tr>	
	  
			
	<tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one" >
              <img src='asserts/FCIT.png' style="width:100%;max-width:100%; position: absolute;top: +5%">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>[14] Federated Continual Instruction Tuning</papertitle>
              <br>
              <i class="fa fa-envelope"></i>
              Haiyang Guo, Fanhu Zeng, Fei Zhu, Wenzhuo Liu, Da-Han Wang, <strong>Jian Xu</strong>, Xu-Yao Zhang, Cheng-Lin Liu 
              <br>
              ICCV 2025
              <br>
          </td>
        </tr>	

	<tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one" >
              <img src='asserts/LongDocURL.png' style="width:100%;max-width:100%; position: absolute;top: +30%">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>[13] LongDocURL: a Comprehensive Multimodal Long Document Benchmark Integrating Understanding, Reasoning, and Locating</papertitle>
              <br>
              <i class="fa fa-envelope"></i>
              Chao Deng, Jiale Yuan, Pi Bu, Peijie Wang, Zhong-Zhi Li, <strong>Jian Xu</strong>, Xiao-Hui Li, Yuan Gao, Jun Song, Bo Zheng, Cheng-Lin Liu
              <br>
              ACL 2025 Main
              <br>
          </td>
        </tr>	
		

	<tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one" >
              <img src='asserts/HiE-VL.png' style="width:100%;max-width:100%; position: absolute;top: +30%">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>[12] HiE-VL: A Large Vision-Language Model with Hierarchical Adapter for Handwritten Mathematical Expression Recognition</papertitle>
              <br>
              <i class="fa fa-envelope"></i>
              Hong-Yu Guo , Fei Yin, <strong>Jian Xu</strong>, Cheng-Lin Liu
              <br>
              ICASSP 2025
              <br>
          </td>
        </tr>	

	<tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one" >
              <img src='asserts/Recoverable_Compression.png' style="width:100%;max-width:100%; position: absolute;top: +20%">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>[11] Recoverable Compression: A Multimodal Vision Token Recovery Mechanism Guided by Text Information</papertitle>
              <br>
              <i class="fa fa-envelope"></i>
              Yi Chen, <strong>Jian Xu</strong>, Xu-Yao Zhang, Wen-Zhuo Liu, Yang-Yang Liu, Cheng-Lin Liu
              <br>
              AAAI 2025
              <br>
              <a href="https://arxiv.org/abs/2409.01179">[PDF]</a>
              <br>
          </td>
        </tr>	

	<tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one" >
              <img src='asserts/StylePrompter.png' style="width:100%;max-width:100%; position: absolute;top: +20%">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>[10] StylePrompter: Enhancing Domain Generalization with Test-Time Style Priors</papertitle>
              <br>
              <i class="fa fa-envelope"></i>
              Jiao Zhang, <strong>Jian Xu</strong>, Xu-Yao Zhang, Cheng-Lin Liu
              <br>
              arXiv 2024
              <br>
              <a href="https://www.arxiv.org/abs/2408.09138">[PDF]</a>
              <br>
          </td>
        </tr>
	  	
	<tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one" >
              <img src='asserts/CMMaTH.png' style="width:100%;max-width:100%; position: absolute;top: +20%">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>[9] CMMaTH: A Chinese Multi-modal Math Skill Evaluation Benchmark for Foundation Models</papertitle>
              <br>
              <i class="fa fa-envelope"></i>
              Zhongzhi Li, Ming-Liang Zhang, Pei-Jie Wang, <strong>Jian Xu</strong>, Rui-Song Zhang, Yin Fei, Zhi-Long Ji, Jin-Feng Bai, Zhen-Ru Pan, Jia-Xin Zhang and Cheng-Lin Liu
              <br>
              COLING 2025
              <br>
              <a href="https://www.arxiv.org/abs/2407.12023">[PDF]</a>
              <br>
              <!--<p>A full-body human motion dataset that captures text-guided desktop object rearrangement through MoCap and AR glasses; & A pipeline for generating avatar's motion of object rearrangement driven by text instruction.</p>-->
          </td>
        </tr>
		
	<tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one" >
              <img src='asserts/favor.png' style="width:100%;max-width:100%; position: absolute;top: +20%">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>[8] FAVOR: Full-Body AR-Driven Virtual Object Rearrangement Guided by Instruction Text</papertitle>
              <br>
              <i class="fa fa-envelope"></i>
              Kailin Li, Lixin Yang, Zenan Lin, <strong>Jian Xu</strong>, Xinyu Zhan, Yifei Zhao, Pengxiang Zhu, Wenxiong Kang, Kejian Wu, Cewu Lu
              <br>
              AAAI 2024
              <br>
              <a href="https://ojs.aaai.org/index.php/AAAI/article/view/28097">[PDF]</a>
              <a href="https://kailinli.github.io/FAVOR/">[Project]</a>
              <br>
              <!--<p>A full-body human motion dataset that captures text-guided desktop object rearrangement through MoCap and AR glasses; & A pipeline for generating avatar's motion of object rearrangement driven by text instruction.</p>-->
          </td>
        </tr>

	  
	<tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one" >
              <img src='asserts/acrpose.png' style="width:100%;max-width:100%; position: absolute;top: +20%">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>[7] ACR-Pose: Adversarial Canonical Representation Reconstruction Network for Category Level 6D Object Pose Estimation</papertitle>
              <br>
              <i class="fa fa-envelope"></i>
              Zhaoxin Fan, Zhenbo Song, Zhicheng Wang, <strong>Jian Xu</strong>, Kejian Wu, Hongyan Liu and Jun He
              <br>
              ICMR 2024
              <br>
              <a href="https://dl.acm.org/doi/10.1145/3652583.3658050">[PDF]</a>
              <br>
            </td>
        </tr>

	  
	<tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one" >
              <img src='asserts/chord.png' style="width:100%;max-width:100%; position: absolute;top: +5%">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>[6] CHORD: Category-level Hand-held Object Reconstruction via Shape Deformation</papertitle>
              <br>
              <i class="fa fa-envelope"></i>
              Kailin Li, Lixin Yang, Haoyu Zhen, Zenan Lin, Xinyu Zhan, Licheng Zhong, <strong>Jian Xu</strong>, Kejian Wu, Cewu Lu
              <br>
              ICCV 2023
              <br>
              <a href="https://arxiv.org/abs/2308.10574">[PDF]</a>
              <a href="https://kailinli.github.io/CHORD/">[Project]</a>
              <br>
              <!--<p>A single-view hand-held object reconstruction method that exploits the categorical shape prior to reconstruct the shape of intra-class objects; & A new synthetic dataset, COMIC, that contains the category-level collection of objects with diverse shape, materials, interacting poses, and viewing directions.</p>-->
          </td>
        </tr>
	  

		
        <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one" >
              <img src='asserts/poem.png' style="width:100%;max-width:100%; position: absolute;top: +20%">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>[5] POEM: Reconstructing Hand in a Point Embedded Multi-view Stereo</papertitle>
              <br>
              <i class="fa fa-envelope"></i>
              Lixin Yang, <strong>Jian Xu</strong>, Licheng Zhong, Xinyu Zhan, Zhicheng Wang, Kejian Wu, Cewu Lu
              <br>
              CVPR 2023
              <br>
              <a href="https://arxiv.org/abs/2304.04038">[PDF]</a>
              <a href="https://github.com/lixiny/POEM/">[Project]</a>
              <br>
              <!--<p>We propose a multi-view hand mesh recovery (HMR) method with Transformer. It leverages the "power of points", including Basis Points Set, point's positional encoding and point-Transformer, to unify and merge information from sparsely arranged cameras.</p>-->
          </td>
        </tr>

        <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one" >
              <img src='asserts/oldnet.png' style="width:100%;max-width:100%; position: absolute;top: +20%">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>[4] Object level depth reconstruction for category level 6d object pose estimation from monocular rgb image</papertitle>
              <br>
              Zhaoxin Fan, Zhenbo Song, <strong>Jian Xu</strong>, Zhicheng Wang, Kejian Wu, Hongyan Liu, Jun He
              <br>
              ECCV 2022
              <br>
              <a href="https://arxiv.org/abs/2204.01586">[PDF]</a>
              <a href="https://github.com/FANzhaoxin666/OLD_Net_release">[Project]</a>
              <br>
              <!--<p>We propose to directly predict object-level depth from a monocular RGB image by deforming the category-level shape prior into object-level depth and the canonical NOCS representation.</p>-->
          </td>
        </tr>

        <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one" >
              <img src='asserts/sba.png' style="width:100%;max-width:100%; position: absolute;top: +20%">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>[3] Unsupervised Semantic-Based Aggregation of Deep Convolutional Features</papertitle>
              <br>
              <strong>Jian Xu</strong>, Chunheng Wang, Cunzhao Shi, Baihua Xiao
              <br>
              IEEE Transactions on Image Processing, 2019
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/8445607">[PDF]</a>
              <a href="https://github.com/XJhaoren/PWA/">[Project]</a>
              <br>
              <!--<p>We propose a simple but effective semantic-based aggregation (SBA) method. </p>-->
          </td>
      </tr>

      <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
              <img src='asserts/ime.png' style="width:100%;max-width:100%; position: absolute;top: +20%">
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>[2] Iterative Manifold Embedding Layer Learned by Incomplete Data for Large-Scale Image Retrieval</papertitle>
              <br>
              <strong>Jian Xu</strong>, Chunheng Wang, Chengzuo Qi, Cunzhao Shi, Baihua Xiao
              <br>
              IEEE Transactions on Multimedia, 2019.
              <br>
              <a href="https://ieeexplore.ieee.org/abstract/document/8566008">[PDF]</a>
              <a href="https://github.com/XJhaoren/IME_layer">[Code]</a>
              <br>
              <!--<p>We propose the iterative manifold embedding (IME) layer, of which the weights are learned offline by an unsupervised strategy, to explore the intrinsic manifolds by incomplete data.</p>-->
          </td>
      </tr>

      <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                  <img src='asserts/pwa.png' style="width:100%;max-width:100%; position: absolute;top: +20%">
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>[1] Unsupervised Part-Based Weighting Aggregation of Deep Convolutional Features for Image Retrieval</papertitle>
              <br>
              <strong>Jian Xu</strong>, Cunzhao Shi, Chengzuo Qi, Chunheng Wang, Baihua Xiao
              <br>
              AAAI 2018
              <br>
              <a href="https://ojs.aaai.org/index.php/AAAI/article/view/12231">[PDF]</a>
              <a href="https://github.com/XJhaoren/PWA">[Code]</a>
              <br>
              <!--<p>We propose a simple but effective semantic part-based weighting aggregation (PWA) for image retrieval.</p>-->
          </td>
      </tr>


	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research Projects</heading>
		  <p style="white-space: nowrap;">[10] 中国人工智能学会蚂蚁科研基金 “原生多模态大模型交互体验升级优化”， 2025.12-2026.12， 项目负责人。</p>
		  <p style="white-space: nowrap;">[9] 国家自然科学基金青年科学基金项目（C类）“多维结构化增强的材料科学大模型构建方法研究”， 2026.01-2028.12， 项目负责人。</p>
		  <p style="white-space: nowrap;">[8] 多模态国家重点实验室青年基金 “基于大模型的科研智能体构建方法研究”， 2025.07-2026.06， 项目负责人。</p>
	      <p style="white-space: nowrap;">[7] 中国科学院先导A “科学基础大模型”， 2025.02-2027.01， 课题负责人。</p>
	      <p style="white-space: nowrap;">[6] 360公司 “面向大语言模型的私域知识注入方法研究”， 2025.03-2025.11， 项目负责人。</p>
	      <p style="white-space: nowrap;">[5] 国家自然科学基金重点项目 “基于神经符号系统的数学推理研究”， 2025.1-2029.12， 主要完成人。 </p>
	      <p style="white-space: nowrap;">[4] 中国科学院先导A “地空多模态甘蔗表型数据智能分析与优异品种选育”，  2023.10-2028.10，  主要完成人。 </p>
          <p style="white-space: nowrap;">[3] 北京市科技计划 “AI for Science重点领域研究案例与智能组件研发”， 2023.12-2025.12， 主要完成人。 </p>
	      <p style="white-space: nowrap;">[2] 2035创新任务 “科学大模型构建理论与方法”， 2024.03-2026.03， 主要完成人。 </p>
	      <p style="white-space: nowrap;">[1] 华为公司 “时序预测大模型高效微调技术研究”， 2024.05-2025.05，  主要完成人。</p>
	    
          </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

	
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:bottom">
                  <heading>Patents</heading>
		  <p style="white-space: nowrap;">[12] <strong>徐健</strong>， 许卓宁，刘成林。甘蔗病害识别方法及装置， 2025-04-17， 发明专利， CN2025104863215。 </p>
          <p style="white-space: nowrap;">[11] <strong>徐健</strong>， 吴克艰，杨理欣。用于确定手部形态的方法、装置、电子设备、介质及产品， 2023-03-31， 发明专利， CN202310344403。 </p>
		  <p style="white-space: nowrap;">[10] <strong>徐健</strong>， 王志成， 吴克艰。用于输出关键点数据的方法、装置、设备、介质及产品， 2022-12-20， 发明专利， CN202211646062。 </p>
		  <p style="white-space: nowrap;">[9] 于杲彤， <strong>徐健</strong>， 王志成， 吴克艰。用于确定手势类型的处理方法、装置、设备和介质， 2022-12-12， 发明专利， CN202211592961。 </p>
		  <p style="white-space: nowrap;">[8] <strong>徐健</strong>， 王志成， 吴克艰。用于头戴显示设备的控制装置、方法、设备和存储介质， 2022-09-07， 发明专利， CN202211089235。 </p>
	      <p style="white-space: nowrap;">[7] <strong>徐健</strong>， 王志成， 吴克艰。用于虚拟键盘的显示方法和装置， 2022-07-15， 发明专利， CN202210833540。 </p>
		  <p style="white-space: nowrap;">[6] <strong>徐健</strong>，  张雅琪， 刘宏马。图像矫正方法、电子设备、介质及片上系统， 2021-09-01， 发明专利， CN202111020078。 </p>
		  <p style="white-space: nowrap;">[5] <strong>徐健</strong>，  张超，  张雅琪， 刘宏马， 贾志平。目标跟踪方法及其装置， 2021-03-29， 发明专利， CN202110336639。 </p>
		  <p style="white-space: nowrap;">[4] 张超，  <strong>徐健</strong>， 张雅琪， 刘宏马， 贾志平，  吕帅林。 一种确定跟踪目标的方法及电子设备，  2020-12-29， 发明专利， CN202011607731， 2023-06-06授权。 </p>
		  <p style="white-space: nowrap;">[3] 张雅琪，  张超，  <strong>徐健</strong>，  刘宏马。一种目标跟踪方法及电子设备， 2020-09-30，  发明专利， CN202011066347。 </p>
		  <p style="white-space: nowrap;">[2] 王春恒，  <strong>徐健</strong>，  肖柏华。 卫星云图分类方法及系统，  2020-06-12，  发明专利， CN202010024821， 2023-04-28授权。 </p>
		  <p style="white-space: nowrap;">[1] 王春恒，  <strong>徐健</strong>，  肖柏华。 图像检索方法及系统， 2020-05-26， 发明专利， CN202010026336， 2023-04-25授权。</p>
          </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
		<hr style="margin-top:0px">
                <p>The website template was adapted from <a href="https://seanchenxy.github.io//">Xingyu Chen</a>.</p>
            </td>
        </tr>

      </td>
    </tr>
  </table>
</body>

</html>
